{"cells":[{"cell_type":"markdown","metadata":{"id":"t89kz3HrrSef"},"source":["# Lets start building the OCR from What we've learned so far"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tcCdyqS8YViE"},"outputs":[],"source":["from google.colab import drive\n","drive.mount(\"/content/drive\")"]},{"cell_type":"markdown","metadata":{"id":"jB2GgTJYnDTg"},"source":["### Import Dependencies"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ajFw5BsKm7Lp"},"outputs":[],"source":["import time\n","import numpy as np\n","import os\n","import pickle\n","import matplotlib.pyplot as plt\n","from keras.datasets import mnist"]},{"cell_type":"markdown","metadata":{"id":"Re3fj-lAnT2p"},"source":["### class LayerDense"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wYt3vBsenJT1"},"outputs":[],"source":["class Layer_Dense:\n","    def __init__(self, n_inputs, n_neurons):\n","        self.weights = 0.10 * np.random.randn(n_inputs, n_neurons)\n","        self.biases = np.zeros((1, n_neurons))\n","\n","    def forward(self, inputs):\n","        self.inputs = inputs\n","        self.output = np.dot(inputs, self.weights) + self.biases\n","\n","    def backward(self, dvalues):\n","        self.dweights = np.dot(self.inputs.T, dvalues)\n","        self.dbiases = np.sum(dvalues, axis=0, keepdims=True)\n","        self.dinputs = np.dot(dvalues, self.weights.T)\n"]},{"cell_type":"markdown","metadata":{"id":"lTKmQ_8IngHA"},"source":["### class ActivationReLU"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LK28K2WonnVR"},"outputs":[],"source":["class ActivationReLU:\n","    def forward(self, inputs):\n","        self.inputs = inputs\n","        self.output = np.maximum(0, inputs)\n","\n","    def backward(self, dvalues):\n","        self.dinputs = dvalues.copy()\n","        self.dinputs[self.inputs <= 0] = 0"]},{"cell_type":"markdown","metadata":{"id":"YT-ENdNTnn12"},"source":["### class ActivationSoftMax"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RtiVzdqsnsLP"},"outputs":[],"source":["class ActivationSoftMax:\n","    def forward(self, inputs):\n","        self.exp_values = np.exp(inputs - np.max(inputs, axis=1, keepdims=True))\n","        self.probabilities = self.exp_values / np.sum(self.exp_values, axis=1, keepdims=True)\n","        self.output = self.probabilities\n","\n","    def backward(self, dvalues):\n","        self.dinputs = np.empty_like(dvalues)\n","        for index, (single_output, single_dvalues) in enumerate(zip(self.output, dvalues)):\n","            single_output = single_output.reshape(-1, 1)\n","            jacobian_matrix = np.diagflat(single_output) - np.dot(single_output, single_output.T)\n","            self.dinputs[index] = np.dot(jacobian_matrix, single_dvalues)"]},{"cell_type":"markdown","metadata":{"id":"JkGEoq4Dns-O"},"source":["### class Loss"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Zi2UkLMrnxOp"},"outputs":[],"source":["class Loss:\n","    def calculate(self, output, y):\n","        self.sample_losses = self.forward(output, y)\n","        self.data_loss = np.mean(self.sample_losses)\n","        return self.data_loss"]},{"cell_type":"markdown","metadata":{"id":"XIxoyEjJnxj7"},"source":["### class LossCategoricalCrossEntropy"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dUVK3lE4oCN0"},"outputs":[],"source":["class LossCategoricalCrossEntropy(Loss):\n","    def forward(self, y_pred, y_true):\n","        samples = len(y_pred)\n","        y_pred_clipped = np.clip(y_pred, 1e-7, 1 - 1e-7)\n","\n","        if y_true.ndim == 1:\n","            correct_confidences = y_pred_clipped[range(samples), y_true]\n","        elif y_true.ndim == 2:\n","            correct_confidences = np.sum(y_pred_clipped * y_true, axis=1)\n","\n","        negative_log_likelihoods = -np.log(correct_confidences)\n","        return negative_log_likelihoods\n","\n","    def backward(self, dvalues, y_true):\n","        samples = len(dvalues)\n","        labels = len(dvalues[0])\n","\n","        if y_true.ndim == 1:\n","            y_true = np.eye(labels)[y_true]\n","\n","        self.dinputs = -y_true / dvalues\n","        self.dinputs = self.dinputs / samples"]},{"cell_type":"markdown","metadata":{"id":"OEqeWaIaoBJ2"},"source":["### class Optimizer_Adam"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gHyG3v3poKmy"},"outputs":[],"source":["class Optimizer_Adam:\n","    def __init__(self, learning_rate=0.001, decay=0.0, epsilon=1e-7,\n","                 beta_1=0.9, beta_2=0.999):\n","        self.learning_rate = learning_rate\n","        self.current_learning_rate = learning_rate\n","        self.decay = decay\n","        self.iterations = 0\n","        self.epsilon = epsilon\n","        self.beta_1 = beta_1\n","        self.beta_2 = beta_2\n","\n","    def pre_update_params(self):\n","        if self.decay:\n","            self.current_learning_rate = self.learning_rate * (1.0 / (1.0 + self.decay * self.iterations))\n","\n","    def update_params(self, layer):\n","        if not hasattr(layer, 'weight_cache'):\n","            layer.weight_momentums = np.zeros_like(layer.weights)\n","            layer.weight_cache = np.zeros_like(layer.weights)\n","            layer.bias_momentums = np.zeros_like(layer.biases)\n","            layer.bias_cache = np.zeros_like(layer.biases)\n","\n","        layer.weight_momentums = self.beta_1 * layer.weight_momentums + (1 - self.beta_1) * layer.dweights\n","        layer.bias_momentums = self.beta_1 * layer.bias_momentums + (1 - self.beta_1) * layer.dbiases\n","\n","        weight_momentums_corrected = layer.weight_momentums / (1 - self.beta_1 ** (self.iterations + 1))\n","        bias_momentums_corrected = layer.bias_momentums / (1 - self.beta_1 ** (self.iterations + 1))\n","\n","        layer.weight_cache = self.beta_2 * layer.weight_cache + (1 - self.beta_2) * layer.dweights**2\n","        layer.bias_cache = self.beta_2 * layer.bias_cache + (1 - self.beta_2) * layer.dbiases**2\n","\n","        weight_cache_corrected = layer.weight_cache / (1 - self.beta_2 ** (self.iterations + 1))\n","        bias_cache_corrected = layer.bias_cache / (1 - self.beta_2 ** (self.iterations + 1))\n","\n","        layer.weights += -self.current_learning_rate * weight_momentums_corrected / (np.sqrt(weight_cache_corrected) + self.epsilon)\n","        layer.biases += -self.current_learning_rate * bias_momentums_corrected / (np.sqrt(bias_cache_corrected) + self.epsilon)\n","\n","    def post_update_params(self):\n","        self.iterations += 1"]},{"cell_type":"markdown","metadata":{"id":"9R01m5ckoKLK"},"source":["### Define layer, activation instances"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wZqiURmcoScx"},"outputs":[],"source":["(X_train, Y_train), (X_test, Y_test) = mnist.load_data()\n","X_train = X_train / 255.0\n","X_test = X_test / 255.0\n","X_train = X_train.reshape(X_train.shape[0], -1)\n","X_test = X_test.reshape(X_test.shape[0], -1)\n","y_train = Y_train\n","y_test = Y_test\n","\n","input_layer = Layer_Dense(784, 128)\n","activation_input = ActivationReLU()\n","\n","hidden_layer1 = Layer_Dense(128, 64)\n","activation1 = ActivationReLU()\n","\n","hidden_layer2 = Layer_Dense(64, 64)\n","activation2 = ActivationReLU()\n","\n","output_layer = Layer_Dense(64, 10)\n","activation_output = ActivationSoftMax()\n","\n","loss_function = LossCategoricalCrossEntropy()\n","optimizer = Optimizer_Adam()\n","\n","losses = []\n","accuracies = []\n","epochs = 60"]},{"cell_type":"markdown","metadata":{"id":"OpbxDFfUoWi3"},"source":["### Training Loop"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8qPXmIqxoWCj"},"outputs":[],"source":["for epoch in range(epochs):\n","    input_layer.forward(X_train)\n","    activation_input.forward(input_layer.output)\n","\n","    hidden_layer1.forward(activation_input.output)\n","    activation1.forward(hidden_layer1.output)\n","\n","    hidden_layer2.forward(activation1.output)\n","    activation2.forward(hidden_layer2.output)\n","\n","    output_layer.forward(activation2.output)\n","    activation_output.forward(output_layer.output)\n","\n","    loss = loss_function.calculate(activation_output.output, y_train)\n","    losses.append(loss)\n","\n","    predictions = np.argmax(activation_output.output, axis=1)\n","    accuracy = np.mean(predictions == y_train)\n","    accuracies.append(accuracy)\n","\n","    loss_function.backward(activation_output.output, y_train)\n","    activation_output.backward(loss_function.dinputs)\n","    output_layer.backward(activation_output.dinputs)\n","\n","    activation2.backward(output_layer.dinputs)\n","    hidden_layer2.backward(activation2.dinputs)\n","\n","    activation1.backward(hidden_layer2.dinputs)\n","    hidden_layer1.backward(activation1.dinputs)\n","\n","    activation_input.backward(hidden_layer1.dinputs)\n","    input_layer.backward(activation_input.dinputs)\n","\n","    optimizer.pre_update_params()\n","    optimizer.update_params(input_layer)\n","    optimizer.update_params(hidden_layer1)\n","    optimizer.update_params(hidden_layer2)\n","    optimizer.update_params(output_layer)\n","    optimizer.post_update_params()\n","\n","    if epoch % 10 == 0:\n","        print(f\"Epoch: {epoch}, Loss: {loss:.3f} Accuracy: {accuracy:.3f}\")"]},{"cell_type":"markdown","metadata":{"id":"dZqhaPWZoeY7"},"source":["Saving Model and Visualising Accuracy over Epochs"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"o3Vel_mkod9D"},"outputs":[],"source":["model_parameters = {\n","    \"input_layer_weights\": input_layer.weights,\n","    \"input_layer_biases\": input_layer.biases,\n","    \"hidden_layer1_weights\": hidden_layer1.weights,\n","    \"hidden_layer1_biases\": hidden_layer1.biases,\n","    \"hidden_layer2_weights\": hidden_layer2.weights,\n","    \"hidden_layer2_biases\": hidden_layer2.biases,\n","    \"output_layer_weights\": output_layer.weights,\n","    \"output_layer_biases\": output_layer.biases,\n","}\n","\n","with open(\"OCR_Model_128,64,64,10.pkl\", \"wb\") as f:\n","    pickle.dump(model_parameters, f)\n","\n","print(\"Model saved successfully.\")\n","\n","plt.figure(figsize=(12, 5))\n","plt.subplot(1, 2, 1)\n","plt.plot(range(epochs), losses, label=\"Loss\")\n","plt.xlabel(\"Epochs\")\n","plt.ylabel(\"Loss\")\n","plt.title(\"Loss over Epochs\")\n","plt.legend()\n","\n","plt.subplot(1, 2, 2)\n","plt.plot(range(epochs), accuracies, label=\"Accuracy\", color=\"orange\")\n","plt.xlabel(\"Epochs\")\n","plt.ylabel(\"Accuracy\")\n","plt.title(\"Accuracy over Epochs\")\n","plt.legend()\n","\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"BSkPaZV3pWZ5"},"source":["# Testing the Saved Model"]},{"cell_type":"markdown","metadata":{"id":"hUdgNDyqqob9"},"source":["### Loading the model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"o2yrnRespaHF"},"outputs":[],"source":["with open('OCR_Model_128,64,64,10.pkl', 'rb') as f:\n","    model_parameters = pickle.load(f)"]},{"cell_type":"markdown","metadata":{"id":"rnJ6GEdeqwDD"},"source":["### Defining Classes without backward propogation and accepting given parameters"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MOJvmkzeqSii"},"outputs":[],"source":["class Layer_Dense:\n","    def __init__(self, n_inputs, n_neurons, weights, biases):\n","        self.weights = weights\n","        self.biases = biases\n","\n","    def forward(self, inputs):\n","        self.inputs = inputs\n","        self.output = np.dot(inputs, self.weights) + self.biases"]},{"cell_type":"markdown","metadata":{"id":"xV4hcBRgrftD"},"source":["### Defining Activation ReLU function"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JC76xacmqVUZ"},"outputs":[],"source":["class ActivationReLU:\n","    def forward(self, inputs):\n","        self.output = np.maximum(0, inputs)"]},{"cell_type":"markdown","metadata":{"id":"V4XoMoSOrn0E"},"source":["### Defining Activation SoftMax Function"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1MfKtDd8qXPF"},"outputs":[],"source":["class ActivationSoftMax:\n","    def forward(self, inputs):\n","        exp_values = np.exp(inputs - np.max(inputs, axis=1, keepdims=True))\n","        probabilities = exp_values / np.sum(exp_values, axis=1, keepdims=True)\n","        self.output = probabilities\n"]},{"cell_type":"markdown","metadata":{"id":"YJaD4rGmrtNa"},"source":["### Prepearing Dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WZHXzfcnqgIV"},"outputs":[],"source":["(X_train, Y_train), (X_test, Y_test) = mnist.load_data()\n","X_test = X_test / 255.0\n","X_test = X_test.reshape(X_test.shape[0], -1)"]},{"cell_type":"markdown","metadata":{"id":"_ahgTDgprw3y"},"source":["### Creating Instances"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fgQBsQi9qg3_"},"outputs":[],"source":["input_layer = Layer_Dense(784, 128, model_parameters[\"input_layer_weights\"], model_parameters[\"input_layer_biases\"])\n","activation_input = ActivationReLU()\n","\n","hidden_layer1 = Layer_Dense(128, 64, model_parameters[\"hidden_layer1_weights\"], model_parameters[\"hidden_layer1_biases\"])\n","activation1 = ActivationReLU()\n","\n","hidden_layer2 = Layer_Dense(64, 64, model_parameters[\"hidden_layer2_weights\"], model_parameters[\"hidden_layer2_biases\"])\n","activation2 = ActivationReLU()\n","\n","output_layer = Layer_Dense(64, 10, model_parameters[\"output_layer_weights\"], model_parameters[\"output_layer_biases\"])\n","activation_output = ActivationSoftMax()"]},{"cell_type":"markdown","metadata":{"id":"8RN8RE06r0eQ"},"source":["### Defining forward propogation function"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Nh8_nj_Yqi9k"},"outputs":[],"source":["def forward_pass(X):\n","    input_layer.forward(X)\n","    activation_input.forward(input_layer.output)\n","\n","    hidden_layer1.forward(activation_input.output)\n","    activation1.forward(hidden_layer1.output)\n","\n","    hidden_layer2.forward(activation1.output)\n","    activation2.forward(hidden_layer2.output)\n","\n","    output_layer.forward(activation2.output)\n","    activation_output.forward(output_layer.output)\n","\n","    return activation_output.output"]},{"cell_type":"markdown","metadata":{"id":"WTp3Y6qQr57_"},"source":["### Visualising the predictions made"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zJ9Yqt--qmrU"},"outputs":[],"source":["def visualize_predictions(X, y_true, y_pred, num_samples=10):\n","    plt.figure(figsize=(10, 10))\n","    for i in range(num_samples):\n","        plt.subplot(1, num_samples, i+1)\n","        plt.imshow(X[i].reshape(28, 28), cmap='gray')\n","        plt.title(f\"True: {y_true[i]}\\nPred: {y_pred[i]}\")\n","        plt.axis('off')\n","    plt.show()\n","\n","for _ in range(5):\n","    indices = np.random.choice(X_test.shape[0], 10, replace=False)\n","    X_sample = X_test[indices]\n","    y_sample = Y_test[indices]\n","\n","    predictions = np.argmax(forward_pass(X_sample), axis=1)\n","\n","    visualize_predictions(X_sample, y_sample, predictions, num_samples=10)"]}],"metadata":{"colab":{"collapsed_sections":["t89kz3HrrSef","jB2GgTJYnDTg","Re3fj-lAnT2p","lTKmQ_8IngHA","YT-ENdNTnn12","JkGEoq4Dns-O","XIxoyEjJnxj7","OEqeWaIaoBJ2","9R01m5ckoKLK","OpbxDFfUoWi3","BSkPaZV3pWZ5","hUdgNDyqqob9","rnJ6GEdeqwDD","xV4hcBRgrftD","V4XoMoSOrn0E","YJaD4rGmrtNa","_ahgTDgprw3y","8RN8RE06r0eQ","WTp3Y6qQr57_"],"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}