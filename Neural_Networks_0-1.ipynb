{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1rW-hI9VqHl2z_4HXi9g_80wq63MkLmoi","timestamp":1723093277729}],"collapsed_sections":["mncx2Xuyi0_g","Jce08PiYi-a5","Xtj1H1_MlVrq","eys3sg-rlh_5","la7GOauPln_w"]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["\n","\n","```\n","# This is formatted as code\n","```\n","\n","# Neural Networks 0-1"],"metadata":{"id":"1bUvDZBnisj6"}},{"cell_type":"markdown","source":["\n","## Lets start with computing output of a single neuron. the formula goes as:\n","\n","$$ \\text{neuron output} = \\mathbf{input} \\cdot \\mathbf{weight} + bias $$\n"],"metadata":{"id":"mYi99AacivWS"}},{"cell_type":"code","source":["w = 0.54623\n","X = 0.67378\n","b = 0.64272\n","nop = w * X + b\n","print(nop)"],"metadata":{"id":"3Q39zWZjUNl-","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1722942255212,"user_tz":420,"elapsed":6,"user":{"displayName":"Asmi Ambekar","userId":"07733780959097800462"}},"outputId":"07271b2a-ed3c-42d4-976f-9ff14f281fa6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["1.0107588494\n"]}]},{"cell_type":"markdown","source":["## Now, lets compute multiple neurons at once, also known as a layer of neurons.\n","\n","$$ \\text{output} = \\sum_{i=0}^{n} (\\text{input}_i \\cdot \\text{weight}_i) + \\text{biases} $$\n","\n","Here input and weight are arrays of float values of which dot product is calculated."],"metadata":{"id":"mncx2Xuyi0_g"}},{"cell_type":"code","source":["import numpy as np"],"metadata":{"id":"Kyon6P1Wi2yl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["X = [0.2,0.4,0.6]\n","w = [0.25,0.45,0.65]\n","b = 0.04\n","op = np.dot(X,w) + b\n","print(op)"],"metadata":{"id":"gyP8tJMHi53R","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1722943049382,"user_tz":420,"elapsed":466,"user":{"displayName":"Asmi Ambekar","userId":"07733780959097800462"}},"outputId":"6618095c-c487-4ff3-a054-75d8a6c5dd41"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["0.6600000000000001\n"]}]},{"cell_type":"markdown","source":["## ReLU Activation Function.\n","$$ \\text{ReLU}(x) = \\max(0, x) $$\n","\n","ReLU is mostly used to activate hidden layers."],"metadata":{"id":"Jce08PiYi-a5"}},{"cell_type":"code","source":["ReLU = np.maximum(0,op)\n","ReLU"],"metadata":{"id":"95wJBiu1i-9F","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1722943895225,"user_tz":420,"elapsed":465,"user":{"displayName":"Asmi Ambekar","userId":"07733780959097800462"}},"outputId":"bea5fc6a-4beb-439c-9f78-b592600584a8"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.6600000000000001"]},"metadata":{},"execution_count":6}]},{"cell_type":"markdown","source":["## SoftMax Activation\n","\n","$$ \\text{Softmax}(z_i) = \\frac{e^{z_i}}{\\sum_{j} e^{z_j}} $$"],"metadata":{"id":"Xtj1H1_MlVrq"}},{"cell_type":"code","source":["inputs = np.random.randn(1,4)\n","inputs\n","exp_values = np.exp(inputs - np.max(inputs, axis=1, keepdims=True))\n","probabilities = exp_values / np.sum(exp_values, axis = 1, keepdims=True)\n","probabilities"],"metadata":{"id":"zsRBJ5B7lcTg","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1722945476622,"user_tz":420,"elapsed":434,"user":{"displayName":"Asmi Ambekar","userId":"07733780959097800462"}},"outputId":"7cbc0a81-2c38-466f-b241-1b28fb5de9f3"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[0.13917295, 0.34603541, 0.27770803, 0.23708361]])"]},"metadata":{},"execution_count":12}]},{"cell_type":"markdown","source":["## Loss Calculation (Categorical Cross-Entropy)\n","\n","$$ \\text{Loss} = -\\sum_{i} y_i \\log(p_i) $$"],"metadata":{"id":"eys3sg-rlh_5"}},{"cell_type":"code","source":["true_value = np.array([0,1,0,0])\n","probabilities = np.clip(probabilities, 1e-15,1 - 1e-15)\n","loss = -(np.sum(true_value * np.log(probabilities)))\n","loss"],"metadata":{"id":"e-iUXao6mVJ5","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1722946688969,"user_tz":420,"elapsed":460,"user":{"displayName":"Asmi Ambekar","userId":"07733780959097800462"}},"outputId":"a0744d13-b22e-4bdc-c044-df320046d4ed"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["1.0612141675210773"]},"metadata":{},"execution_count":15}]},{"cell_type":"markdown","source":["## Adam Optimizer\n","\n","### Compute the biased first moment estimate $m_t$\n","$$\n","m_t = \\beta_1 \\cdot m_{t-1} + (1 - \\beta_1) \\cdot g_t\n","$$\n","\n","- where $g_t$ is the gradient at time step $t$.\n","\n","### Compute the biased second raw moment estimate $v_t$:\n","\n","$$\n","v_t = \\beta_2 \\cdot v_{t-1} + (1 - \\beta_2) \\cdot g_t^2\n","$$\n","### Compute bias-corrected first moment estimate $\\hat{m}^t$:\n","\n","$$\\hat{m}_t = \\frac{m_t}{1 - \\beta_1^t}$$\n","\n","### Compute bias-corrected second raw moment estimate $\\hat{v}^t$:\n","$$m_t = \\beta_1 \\cdot m_{t-1} + (1 - \\beta_1) \\cdot g_t $$\n","\n","### Update the parameters $Î¸$:\n","$$\\theta_t = \\theta_{t-1} - \\frac{\\alpha \\cdot \\hat{v}_t}{\\sqrt{\\hat{m}_t} + \\epsilon}$$"],"metadata":{"id":"la7GOauPln_w"}},{"cell_type":"code","source":[],"metadata":{"id":"Ii8Qjmw3lnKv"},"execution_count":null,"outputs":[]}]}